{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spectra extraction and psf matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following codes extract the line maps from the existing data products and performs psf matching\n",
    "the extracted files are saved following the prefix .extract.fits \n",
    "the entries of each fits files:\n",
    "\n",
    "0 PRIMARY\n",
    "\n",
    "1 ZFIT_STACK\n",
    "\n",
    "2 SEG\n",
    "\n",
    "3 DSCI\n",
    "\n",
    "4 LINE_HA\n",
    "\n",
    "5 LINEWHT_HA\n",
    "\n",
    "6 LINE_HB\n",
    "\n",
    "7 LINEWHT_HB\n",
    "\n",
    "8 PSF_HA\n",
    "\n",
    "9 PSF_HB\n",
    "\n",
    "10 PSF_MATCH\n",
    "\n",
    "11 LINE_HB_CONVOLVED\n",
    "\n",
    "12 LINWHT_HB_CONVOLVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from    astropy.table       import Table\n",
    "import  numpy               as     np\n",
    "from    astropy.io          import fits\n",
    "import  webbpsf\n",
    "from    photutils.psf       import matching as match\n",
    "from    astropy.convolution import convolve_fft \n",
    "from    tqdm                import tqdm\n",
    "import  os\n",
    "import  gc                                         \n",
    "\n",
    "\n",
    "#this deals with warnings\n",
    "import warnings                                   \n",
    "from astropy.io.fits.verify import VerifyWarning                          \n",
    "warnings.simplefilter('ignore', VerifyWarning)\n",
    "warnings.simplefilter('ignore', UserWarning) \n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "#this is just a handy little function to return the desired file path\n",
    "#give one entry in the objectlist, return the desired file path\n",
    "def file_path(obj,prefix,filetype='fits'):\n",
    "    if   filetype == 'fits':\n",
    "        return f\"data/{obj['field']}/{obj['field']}_{str(obj['id']).zfill(5)}.{prefix}.{filetype}\"\n",
    "    elif filetype == 'png':\n",
    "        return f\"png/{obj['field']}/{obj['field']}_{str(obj['id']).zfill(5)}.{prefix}.{filetype}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract ha hb line maps from full.fits data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exrtract_HaHb(hdu):\n",
    "    \"\"\"\n",
    "\n",
    "    pass objs from obj_lis to extract ha hb lines\n",
    "\n",
    "    return: HDUlist with the following entry:\n",
    "\n",
    "    0 primary extension, same as original file\n",
    "\n",
    "    1 line-fit results\n",
    "\n",
    "    2 segmentation map\n",
    "\n",
    "    3 clear filter maps\n",
    "\n",
    "    4,5 Ha line map & line weight\n",
    "\n",
    "    6,7 Hb line map & line weight\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    #set up a crop of 50x50 pix in the center\n",
    "    center_size = 50; shape = hdu[5].shape[0]\n",
    "    #start index: si and end index: ei\n",
    "    si = (shape - center_size) // 2; \n",
    "    ei = si + center_size\n",
    "\n",
    "\n",
    "    new_file = fits.HDUList()\n",
    "    #save primary extension\n",
    "    new_file.append(hdu[0])\n",
    "    #save line-fit info\n",
    "    new_file.append(hdu[1])\n",
    "    \"\"\"\n",
    "    select segmentation map[4]\n",
    "    also save 1 DSCI image for comparison [5]\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in [4,5]: \n",
    "        hdu[i].data = hdu[i].data[si:ei,si:ei]\n",
    "        new_file.append(hdu[i])\n",
    "\n",
    "    #loop to select ha hb line maps\n",
    "    for image in hdu:\n",
    "        if image.ver in ['Ha','Hb'] and (image.name == 'LINE' or image.name == 'LINEWHT'):\n",
    "            image.data = image.data[si:ei,si:ei]\n",
    "            image.name = f'{image.name}_{image.ver}'\n",
    "            new_file.append(image)\n",
    "    return new_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate monochromatic PSFs for Ha Hb line maps, generating kernels for Hb psf matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_data\n",
    "NIRISS_filters = [\n",
    "    {\"name\": \"F090W\", \"lambda_min\": 0.796, \"lambda_max\": 1.005},\n",
    "    {\"name\": \"F115W\", \"lambda_min\": 1.013, \"lambda_max\": 1.283},\n",
    "    {\"name\": \"F150W\", \"lambda_min\": 1.330, \"lambda_max\": 1.671},\n",
    "    {\"name\": \"F200W\", \"lambda_min\": 1.751, \"lambda_max\": 2.226},\n",
    "    {\"name\": \"F277W\", \"lambda_min\": 2.413, \"lambda_max\": 3.143},\n",
    "    {\"name\": \"F356W\", \"lambda_min\": 3.140, \"lambda_max\": 4.068},\n",
    "    {\"name\": \"F444W\", \"lambda_min\": 3.880, \"lambda_max\": 5.023}\n",
    "]\n",
    "\n",
    "#calculate mono psf\n",
    "def mono_webbpsf(wavelength,filter):\n",
    "    niriss = webbpsf.NIRISS()\n",
    "    niriss.filter = filter\n",
    "    return niriss.calc_psf(fov_pixels=20,monochromatic=wavelength)[3]\n",
    "\n",
    "def calc_psf(z):\n",
    "    #use redshift to get line wave length\n",
    "    waves = [6.563e-7 *(1+z), 4.861e-7 *(1+z)]\n",
    "    filters = []\n",
    "    #locate filter for Ha Hb linemaps\n",
    "    for wave in waves:\n",
    "        for f in NIRISS_filters:\n",
    "            if f[\"lambda_min\"] <= wave*1e6 <= f[\"lambda_max\"]:\n",
    "                filters.append( f\"{f['name']}\")\n",
    "                continue\n",
    "    if len(filters) == 2:\n",
    "        return mono_webbpsf(waves[0],filters[0]), mono_webbpsf(waves[1],filters[1])\n",
    "    else:\n",
    "        #return 0,0 when failed to loacte filter\n",
    "        return [0,0]\n",
    "\n",
    "#generate kernel for convolving hb for psf matching\n",
    "def gen_kernel(psf_hb,psf_ha):\n",
    "    #generate kernel to match psf_Hb to psf_Ha\n",
    "        kernel = fits.ImageHDU()\n",
    "        window = match.CosineBellWindow(alpha=1.25)\n",
    "        kernel.data = match.create_matching_kernel(psf_hb.data,psf_ha.data,window=window)\n",
    "        kernel.name = 'PSF_MATCH'\n",
    "        return kernel\n",
    "\n",
    "#convolve hb line maps to match psf of ha\n",
    "def psf_convolve(hb,kernel):\n",
    "    return fits.ImageHDU(\n",
    "        data = convolve_fft(hb.data,kernel.data),\n",
    "        header = hb.header,\n",
    "        name = 'LINE_HB_CONVOLVED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Table: 100%|███████████████████████████████████████████| 752/752 [11:14<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of objects before psf matching 752\n",
      "number of objects after  psf matching 692\n"
     ]
    }
   ],
   "source": [
    "#list of obj selected \n",
    "obj_lis = Table.read('spectra-fitting_selected.fits')\n",
    "row_to_remove = []\n",
    "\n",
    "step = 0 #every 10 steps clear garbage \n",
    "\n",
    "for index in tqdm(range(len(obj_lis)),\n",
    "                desc=\"Processing Table\",\n",
    "                ncols=100):\n",
    "\n",
    "    obj=obj_lis[index]\n",
    "    #if os.path.exists(file_path(obj=obj,prefix='extracted')):\n",
    "    #    continue\n",
    "\n",
    "    path = file_path(obj,'full')\n",
    "    with fits.open(path) as hdu:\n",
    "    \n",
    "        #extract HaHb from full data\n",
    "        extracted = exrtract_HaHb(hdu)\n",
    "        \n",
    "        #calculate monochromatic psf\n",
    "        #only proceed if psf is correctly calculated           \n",
    "        psf_ha,psf_hb =calc_psf(extracted[0].header['redshift'])\n",
    "        if psf_ha != 0 and psf_hb !=0:\n",
    "            psf_ha.name = 'PSF_HA'\n",
    "            psf_hb.name = 'PSF_HB'\n",
    "\n",
    "            #generate kernel and convolve\n",
    "            kernel = gen_kernel(psf_hb,psf_ha)\n",
    "            hb_convolve =  psf_convolve(extracted[6],kernel)\n",
    "            #also convolve hb error map\n",
    "            hb_err = 1/np.sqrt(extracted[7].data)\n",
    "            hb_weight_convolved = fits.ImageHDU(data = convolve_fft(extracted[7].data,kernel.data),\n",
    "                                            header=hdu[7].header,\n",
    "                                            name='LINEWHT_HB_convolved')\n",
    "            \n",
    "            extracted.append(psf_ha)\n",
    "            extracted.append(psf_hb)\n",
    "            extracted.append(kernel)\n",
    "            extracted.append(hb_convolve) \n",
    "            extracted.append(hb_weight_convolved)\n",
    "\n",
    "            rewrite = file_path(obj=obj,prefix='extracted')\n",
    "            extracted.writeto(rewrite,overwrite=True)\n",
    "                \n",
    "        else:\n",
    "            row_to_remove.append(index)   \n",
    "        step += 1\n",
    "        if step >= 10:\n",
    "            gc.collect()   \n",
    "            step=0 \n",
    "\n",
    "print('number of objects before psf matching',len(obj_lis))\n",
    "#remove rows with unsuccessful psf matching\n",
    "obj_lis.remove_rows(row_to_remove)\n",
    "print('number of objects after  psf matching',len(obj_lis))\n",
    "obj_lis.write('spectra-fitting_selected_psfmatched.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
